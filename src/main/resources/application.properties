spring.application.name=springai-ollama-demo
server.port=8081
spring.ai.ollama.base-url=http://localhost:11434

#spring.ai.ollama.chat.options.model=phi
spring.ai.ollama.chat.options.model=mistral
#spring.ai.ollama.chat.options.temperature=0.7

#spring.ai.ollama.chat.model=phi
#spring.ai.ollama.chat.model=llama2
#spring.ai.ollama.chat.model=mistral

#Chatbot Ollama
#DEFAULT_MODEL="codellama:latest"
#NEXT_PUBLIC_DEFAULT_SYSTEM_PROMPT=""